# yaml-language-server: $schema=https://raw.githubusercontent.com/datreeio/CRDs-catalog/refs/heads/main/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  chart:
    spec:
      chart: kube-prometheus-stack
      reconcileStrategy: ChartVersion
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
      version: 78.x.x
  install:
    crds: Create
    createNamespace: true
  interval: 1m
  targetNamespace: monitoring
  upgrade:
    crds: CreateReplace
  values:
    # Install Prometheus Operator CRDs
    crds:
      enabled: true
    # Create default rules for monitoring the cluster
    defaultRules:
      # create default rules
      create: true
      # disable the ones that are not needed
      rules:
        etcd: false
        windows: false
      # Disabled PrometheusRule alerts
      disabled: {}
    # Provide custom recording or alerting rules to be deployed into the cluster.
    additionalPrometheusRulesMap: {}
    # Configuration for alertmanager
    alertmanager:
      enabled: true
      # Alertmanager configuration directives
      # ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
      #      https://prometheus.io/webtools/alerting/routing-tree-editor/
      config:
        global:
          resolve_timeout: 5m
        route:
          group_by: ['namespace']
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 12h
          receiver: 'null'
          routes:
            - receiver: 'null'
              matchers:
                - alertname = "Watchdog"
            - receiver: 'ntfy'
              continue: true
        receivers:
          - name: 'null'
          - name: 'ntfy'
            webhook_configs:
              - send_resolved: true
                url: http://alertfy-svc.ntfy.svc.cluster.local/hook
      alertmanagerSpec:
        # Size is the expected size of the alertmanager cluster. The controller
        # will eventually make the size of the running cluster equal to the
        # expected size.
        replicas: 1
        # Time duration Alertmanager shall retain data for. Default is '120h'.
        retention: 24h
        # Storage is the definition of how storage will be used by the
        # Alertmanager instances.
        storage:
          volumeClaimTemplate:
            spec:
              # use default storage class
              storageClassName: null
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 1Gi
        # Define resources requests and limits for single Pods.
        resources:
          requests:
            cpu: 20m
            memory: 25Mi
          limits:
            cpu: 20m
            memory: 50Mi
    # Configuration for grafana
    grafana:
      enabled: true
      # Deploy default dashboards
      defaultDashboardsEnabled: true
      # Timezone for the default dashboards
      # Other options are: browser or a specific timezone, i.e.
      # Europe/Luxembourg
      defaultDashboardsTimezone: browser
      # Editable flag for the default dashboards
      defaultDashboardsEditable: true
      adminPassword: ihategrafana
      resources:
        requests:
          cpu: 200m
          memory: 300Mi
        limits:
          cpu: 200m
          memory: 400Mi
      # fix for https://github.com/prometheus-community/helm-charts/issues/3800
      serviceMonitor:
        labels:
          release: kube-prometheus-stack
      ingress:
        # If true, Grafana Ingress will be created
        enabled: true
        # IngressClassName for Grafana Ingress.
        # Should be provided if Ingress is enable.
        ingressClassName: traefik
        # Annotations for Grafana Ingress
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt-prod
        # Labels to be added to the Ingress
        labels: {}
        # Hostnames.
        # Must be provided if Ingress is enable.
        hosts:
          - grafana.murtazau.xyz
        # Path for grafana ingress
        path: /
        # TLS configuration for grafana Ingress
        # Secret must be manually created in the namespace
        tls:
          - secretName: grafana-tls
            hosts:
              - grafana.murtazau.xyz
      # To make Grafana persistent (Using Statefulset)
      persistence:
        enabled: true
        type: sts
        # use default storage class
        storageClassName: null
        accessModes:
          - ReadWriteOnce
        size: 3Gi
        finalizers:
          - kubernetes.io/pvc-protection
      sidecar:
        dashboards:
          enabled: true
          label: grafana_dashboard
          labelValue: "1"
          # Allow discovery in all namespaces for dashboards
          searchNamespace: ALL
          provider:
            allowUiUpdates: true
        datasources:
          enabled: true
          defaultDatasourceEnabled: true
          label: grafana_datasource
          labelValue: "1"
          alertmanager:
            enabled: false
      ## Configure additional grafana datasources (passed through tpl)
      ## ref: http://docs.grafana.org/administration/provisioning/#datasources
      additionalDataSources:
        - name: loki
          access: proxy
          basicAuth: false
          editable: false
          type: loki
          url: "http://monitoring-loki-gateway.monitoring.svc.cluster.local"
      # Flag to mark provisioned data sources for deletion if they are no
      # longer configured. It takes no effect if data sources are already
      # listed in the deleteDatasources section.
      #
      # ref: https://grafana.com/docs/grafana/latest/administration/provisioning/#example-data-source-config-file
      prune: true
    # Flag to disable all the kubernetes component scrapers
    kubernetesServiceMonitors:
      enabled: true
    # Component scraping the kube api server
    kubeApiServer:
      enabled: true
    # Component scraping the kubelet and kubelet-hosted cAdvisor
    kubelet:
      enabled: true
      namespace: kube-system
    # Component scraping the kube controller manager
    kubeControllerManager:
      enabled: true
      # If your kube controller manager is not deployed as a pod, specify IPs
      # it can be found on
      endpoints:
        - 10.0.0.10
      service:
        enabled: true
        port: 10257
        targetPort: 10257
      serviceMonitor:
        enabled: true
        https: true
        insecureSkipVerify: true
    # Component scraping coreDns. Use either this or kubeDns
    coreDns:
      enabled: true
    # Component scraping etcd
    #
    # Disabled because we are using K3S embedded sqlite datastore instead of
    # etcd.
    kubeEtcd:
      enabled: false
    # Component scraping kube scheduler
    kubeScheduler:
      enabled: true
      # If your kube scheduler is not deployed as a pod, specify IPs it can be
      # found on
      endpoints:
        - 10.0.0.10
      service:
        enabled: true
        port: 10259
        targetPort: 10259
      serviceMonitor:
        enabled: true
        https: true
        insecureSkipVerify: true
    # Component scraping kube proxy
    kubeProxy:
      enabled: true
      # If your kube proxy is not deployed as a pod, specify IPs it can be found on
      endpoints:
        - 10.0.0.10
        - 10.0.0.11
      service:
        enabled: true
        port: 10249
        targetPort: 10249
        selector:
          k8s-app: kube-proxy
    # Component scraping kube state metrics
    kubeStateMetrics:
      enabled: true
    # Deploy node exporter as a daemonset to all nodes
    nodeExporter:
      enabled: true
      operatingSystems:
        linux:
          enabled: true
        aix:
          enabled: false
        darwin:
          enabled: false
    prometheus-node-exporter:
      tolerations:
        - key: availability
          operator: Equal
          value: low
          effect: NoExecute
    # Manages Prometheus and Alertmanager components
    prometheusOperator:
      enabled: true
      # Number of old replicasets to retain The default value is 10, 0 will
      # garbage-collect old replicasets
      revisionHistoryLimit: 3
      networkPolicy:
        # Enable creation of NetworkPolicy resources.
        enabled: false
      kubeletService:
        # If true, the operator will create and maintain a service for scraping
        # kubelets
        #
        # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md
        enabled: true
        namespace: kube-system
        selector: ""
        # Use '{{ template "kube-prometheus-stack.fullname" . }}-kubelet' by default
        name: ""
      # Create Endpoints objects for kubelet targets.
      kubeletEndpointsEnabled: true
      # Create EndpointSlice objects for kubelet targets.
      kubeletEndpointSliceEnabled: false
      # Resource limits & requests
      resources:
        requests:
          cpu: 10m
          memory: 25Mi
        limits:
          cpu: 20m
          memory: 60Mi
    # Deploy a Prometheus instance
    prometheus:
      enabled: true
      # Toggle prometheus into agent mode
      # Note many of features described below (e.g. rules, query, alerting, remote
      # read, thanos) will not work in agent mode.
      #
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/designs/prometheus-agent.md
      agentMode: false
      # Configure network policy for the prometheus
      networkPolicy:
        enabled: false
      # Settings affecting prometheusSpec
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
      prometheusSpec:
        # Statefulset's persistent volume claim retention policy whenDeleted and
        # whenScaled determine whether statefulset's PVCs are deleted (true) or
        # retained (false) on scaling down and deleting statefulset, respectively.
        # Requires Kubernetes version 1.27.0+.
        # Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention
        persistentVolumeClaimRetentionPolicy:
          whenDeleted: Retain
          whenScaled: Retain
        disableCompaction: false
        # Interval between consecutive scrapes.
        # Defaults to 30s.
        # ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183
        scrapeInterval: "30s"
        # Number of seconds to wait for target to respond before erroring
        scrapeTimeout: ""
        # How long to retain metrics
        retention: 168h # 7d
        # Maximum size of metrics
        retentionSize: ""
        # Number of replicas of each shard to deploy for a Prometheus deployment.
        # Number of replicas multiplied by shards is the total number of Pods
        # created.
        replicas: 1
        # Resource limits & requests
        resources:
          requests:
            cpu: 320m
            memory: 600Mi
          limits:
            cpu: 320m
            memory: 1Gi
        # Prometheus StorageSpec for persistent data
        # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
        storageSpec:
          volumeClaimTemplate:
            spec:
              # use default storage class
              storageClassName: null
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 35Gi
